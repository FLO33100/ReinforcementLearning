{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state=None; mask=None; deterministic=True\n",
    "\n",
    "\n",
    "obs = env.reset()\n",
    "for i in range(env.envs[0].total_window -env.envs[0].lookback_window-1):\n",
    "#     i += 1\n",
    "#     print(i)\n",
    "#     import pdb;pdb.set_trace()\n",
    "#     action, _states = model.predict(obs, deterministic=False)\n",
    "    \n",
    "    observation = np.array(obs)\n",
    "    vectorized_env = model._is_vectorized_observation(observation, model.observation_space)\n",
    "\n",
    "    observation = observation.reshape((-1,) + model.observation_space.shape)\n",
    "    with model.sess.as_default():\n",
    "        \n",
    "        \n",
    "        \n",
    "#         actions, _, _ = model.step_model.step(observation, deterministic=deterministic)\n",
    "        \n",
    "        q_values, actions_proba = model.step_model.sess.run([model.step_model.q_values, model.step_model.policy_proba], {model.step_model.obs_ph: obs})\n",
    "        print(actions_proba)\n",
    "        print(q_values)\n",
    "        if deterministic:\n",
    "            actions = np.argmax(q_values, axis=1)\n",
    "        else:\n",
    "            # Unefficient sampling\n",
    "            # TODO: replace the loop\n",
    "            # maybe with Gumbel-max trick ? (http://amid.fish/humble-gumbel)\n",
    "            actions = np.zeros((len(obs),), dtype=np.int64)\n",
    "            for action_idx in range(len(obs)):\n",
    "                actions[action_idx] = np.random.choice(model.step_model.n_actions, p=actions_proba[action_idx])\n",
    "        actions, _, _ = actions, q_values, None\n",
    "\n",
    "    if not vectorized_env:\n",
    "        actions = actions[0]\n",
    "    \n",
    "    action, _states = actions, None\n",
    "    \n",
    "    obs, rewards, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_obs, df_prices = df_obs_and_prices(num, asset)\n",
    "df_prices = propagate_index(from_df=df_obs, to_df=df_prices_all)\n",
    "\n",
    "env = DummyVecEnv([lambda: AequamEnv(df_obs.iloc[::n_filter], df_prices.iloc[::n_filter],\\\n",
    "                                     lookback_window = lookback_window, reward_type = reward_type,\\\n",
    "                                     transaction_smoothing = transaction_smoothing)])\n",
    "\n",
    "if algo == 'dqn':   \n",
    "    model = DQN(DQN_MlpPolicy, env, param_noise=True, verbose=1, tensorboard_log='tmp/')\n",
    "elif algo == 'ppo2' :   \n",
    "    model = PPO2(MlpPolicy, env, verbose=1, tensorboard_log='tmp/')\n",
    "#     total_timesteps *= 100\n",
    "else:\n",
    "    print(1/0)\n",
    "    \n",
    "model.learn(total_timesteps=total_timesteps)\n",
    "\n",
    "env.envs[0].play_last_episode(model)\n",
    "\n",
    "env.envs[0].print_pdf_report('test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
